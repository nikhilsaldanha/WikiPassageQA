{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "Stemmer=SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DIR = os.path.abspath(\"\")\n",
    "DATA_DIR = os.path.join(CUR_DIR, \"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    str=remove_stopwords(str)\n",
    "    words = word_tokenize(str)\n",
    "    #removing stop words\n",
    "    for word in words:\n",
    "        #word = Stemmer.stem(word)\n",
    "        word= wordnet_lemmatizer.lemmatize(word)\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_path = os.path.join(DATA_DIR, \"raw/WikiPassageQA/document_passages.json\")\n",
    "with open(passages_path) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = os.path.join(DATA_DIR, \"raw/extracted_query_data/train_exp.csv\")\n",
    "df = pd.read_csv(query_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid=str(df.iloc[0][0])\n",
    "relp = str(df.iloc[0][4])\n",
    "question=df.iloc[0][3].translate(table).lower()\n",
    "passage=data[docid][relp].translate(table).lower()\n",
    "passage=re.sub(r'\\n', ' ', passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = word_count(question)\n",
    "p = word_count(passage)\n",
    "setA = set(q)\n",
    "setB = set(p)\n",
    "print(\"The overlapping words between question and passage is:- \")\n",
    "print(setA.intersection(setB))\n",
    "print(\"Total number of overlapping words: \"+str(len(setA.intersection(setB))))\n",
    "print(\"Counts of the overlapping words in question and passage:-\")\n",
    "for item in setA.intersection(setB):\n",
    "    print(\"Q count for the word '\"+ item +\"' is \"+str(q[item]))\n",
    "    print(\"P count for the word '\"+ item +\"' is \"+str(p[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=[]\n",
    "Qcounts=[]\n",
    "Pcounts=[]\n",
    "for i in range(len(df)):\n",
    "    docid=str(df.iloc[i][0])\n",
    "    relp = str(df.iloc[i][4])\n",
    "    question=df.iloc[i][3].translate(table).lower()\n",
    "    passage=data[docid][relp].translate(table).lower()\n",
    "    passage=re.sub(r'\\n', ' ', passage)\n",
    "    q = word_count(question)\n",
    "    p = word_count(passage)\n",
    "    setA = set(q)\n",
    "    setB = set(p)\n",
    "    intersect = setA.intersection(setB)\n",
    "    resQ = {key: q[key] for key in intersect} \n",
    "    resP = {key: p[key] for key in intersect}\n",
    "    Qcounts.append(resQ)\n",
    "    Pcounts.append(resP)\n",
    "    counts.append(len(intersect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.array(counts)\n",
    "counts.mean(),counts.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Qcounts[0]),list(Pcounts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing for the overlapping words how much they occur in the passages on average. \n",
    "means=[]\n",
    "stds=[]\n",
    "for i in range(len(Pcounts)):\n",
    "    if len(list(Pcounts[i]))!=0:\n",
    "        means.append(np.array(list(Pcounts[i].values())).mean())\n",
    "        stds.append(np.array(list(Pcounts[i].values())).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(means).mean(),np.array(means).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(stds).mean(),np.array(stds).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_lengths=[]\n",
    "for i in range(len(df)):\n",
    "    docid=str(df.iloc[i][0])\n",
    "    relp = str(df.iloc[i][4])\n",
    "    passage=data[docid][relp].translate(table).lower()\n",
    "    passage=re.sub(r'\\n', ' ', passage)\n",
    "    passage=remove_stopwords(passage)\n",
    "    words = word_tokenize(passage)\n",
    "    passage_lengths.append(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = np.array(list(Pcounts[0].values()))/passage_lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freqs=[]\n",
    "for i in range(len(Pcounts)):\n",
    "    term_freqs.append(np.array(list(Pcounts[i].values()))/passage_lengths[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing for the overlapping words, the term frequencies in the passages on average. \n",
    "means_tf=[]\n",
    "stds_tf=[]\n",
    "for i in range(len(term_freqs)):\n",
    "    if len(list(term_freqs[i]))!=0:\n",
    "        means_tf.append(term_freqs[i].mean())\n",
    "        stds_tf.append(term_freqs[i].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(means_tf).mean(),np.array(means_tf).std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(stds_tf).mean(),np.array(stds_tf).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpassages=[]\n",
    "for i in data.keys():\n",
    "    for j in data[i].keys():\n",
    "        allpassages.append(data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dict()      \n",
    "for dicts in Pcounts:\n",
    "    for word in list(dicts.keys()):        \n",
    "        count=0\n",
    "        for passages in allpassages:\n",
    "            if word in passages:\n",
    "                count+=1\n",
    "                continue\n",
    "        counts[word]=count        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[\"evangelicalism\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "doc_freq_path = os.path.join(DATA_DIR, \"processed/DocumentFreq.pickle\")\n",
    "with open(doc_freq_path, 'wb') as handle:\n",
    "    pickle.dump(counts, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evangelicalismidf=np.log(50612/(counts[\"evangelicalism\"]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[0]*evangelicalismidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freqs[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termfreq_idf=[]\n",
    "for i,j in zip(term_freqs,Pcounts):\n",
    "    templ=[]\n",
    "    for k,l in zip(i,j.keys()):\n",
    "        templ.append(k*np.log(50612/(counts[l]+1)))\n",
    "        \n",
    "    termfreq_idf.append(np.array(templ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termfreq_idf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing for the overlapping words how much they occur with respect to term freq_idf in the passages on average. \n",
    "means_tf_idf=[]\n",
    "stds_tf_idf=[]\n",
    "for i in range(len(termfreq_idf)):\n",
    "    if len(list(termfreq_idf[i]))!=0:\n",
    "        means_tf_idf.append(termfreq_idf[i].mean())\n",
    "        stds_tf_idf.append(termfreq_idf[i].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(means_tf_idf).mean(),np.array(means_tf_idf).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(stds_tf_idf).mean(),np.array(stds_tf_idf).std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
